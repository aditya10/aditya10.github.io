I"&<p>üëã Hello there!</p>

<p>I work at the Computer Vision Lab at University of British Columbia with <a href="https://www.cs.ubc.ca/~lsigal/index.html">Dr. Leonid Sigal</a> and <a href="https://lrjconan.github.io">Dr. Renjie Liao</a>. My interests lie in multimodal vision-language models, time-series models, and commonsense reasoning. Currently, I am an intern at <a href="https://www.borealisai.com">Borealis AI</a>, working with <a href="https://www.borealisai.com/team-member/fred-tung/">Dr. Fred Tung</a>.</p>

<p>In addition, I am a photographer in the city. I enjoy doing landscape and product photography. My work has over <a href="https://unsplash.com/@adityachinchure">100 million views on Unsplash</a>. You can find my latest work on <a href="https://www.instagram.com/aditya.chinchure/">Instagram</a>.</p>

<p>If you are reading this, I would love to talk to you! I am always looking for opportunities to collaborate. Also, my inbox is open if you have any questions about student life at UBC or Vancouver. Message me on <a href="https://www.instagram.com/aditya.chinchure/">Instagram</a> or send me an <a href="mailto:aditya.chinchure+web@gmail.com">email</a>.</p>

<h2 id="-publications">üìö Publications</h2>

<p><strong>VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge</strong> <br />
Sahithya Ravi*, <strong>Aditya Chinchure</strong>*, Leonid Sigal, Renjie Liao, Vered Shwartz (* equal) <br />
<em>Accepted at WACV 2023</em> <br />
<a href="https://arxiv.org/abs/2210.13626">arXiv</a> | <a href="https://github.com/aditya10/VLC-BERT">Code</a></p>

<p><strong>Refinement Architectures for Referring Image Segmentation [Honours Thesis]</strong> <br />
<strong>Aditya Chinchure</strong> <br />
<a href="https://drive.google.com/file/d/1cU3ysSpXoYRvUslg4RIENoS7O3-lV0sb/view?usp=sharing">Thesis</a></p>

<p><strong>LEAP: Private and Federated Data Analysis for Healthcare</strong> <br />
Matheus Stolet, Chris Yoon, Kalli Leung, <strong>Aditya Chinchure</strong>, Mathias L√©cuyer, Aline Talhouk, Ivan Beschastnikh <br />
<em>Poster at Emerging Technologies: BC‚Äôs AI Showcase, organized by UBC‚Äôs Centre for Artificial Intelligence Decision-making and Action (CAIDA)</em> <br />
<a href="https://leap-project.github.io">Website</a></p>

<h2 id="-work">üë®‚Äçüíª Work</h2>

<p><strong>Research Intern, Borealis AI, RBC</strong> <br />
<em>Vancouver | September 2022 - Present</em> <br />
Working on event time-series representation learning with transformers.</p>

<p><strong>Graduate Research Assistant, Computer Vision Lab at UBC Computer Science</strong> <br />
<em>Vancouver | May 2022 - Present</em> <br />
Working on Visual Question Answering with external commonsense knowledge.</p>

<p><strong>Undergraduate Research Assistant, Computer Vision Lab at UBC</strong> <br />
<em>Vancouver | May 2020 - August 2020</em> <br />
Working on structured attention for vision-text transformer models to improve image grounding.</p>

<p><strong>Undergraduate Research Assistant, LEAP Project at UBC Computer Science</strong> <br />
<em>Vancouver | May 2020 - August 2020</em> <br />
Working on backend projects for LEAP, a differential privacy-focused federated data analytics platform.</p>

<p><strong>Machine Learning Engineer (Co-op), Hypercontext (prev. SoapBox)</strong> <br />
<em>Toronto | May 2019 - August 2019</em> <br />
Developed machine learning models for text classification, sentiment analysis and entity recognition using PyTorch, fast.ai and RASA NLU. This work is used in the <a href="https://hypercontext.com/features/meeting-insights">Meeting Insights</a> feature of the product!</p>

<p><strong>Junior Software Developer (Co-op), AppNeta</strong> <br />
<em>Vancouver | September 2018 ‚Äì April 2019</em> <br />
Worked in a team of eight to scale up our application for cloud deployments.</p>

<h2 id="-education">üë®‚Äçüéì Education</h2>

<p><strong>MSc. in Computer Science</strong> <br />
The University of British Columbia |
2021 - 2023</p>

<p><strong>BSc. Honours in Computer Science</strong> <br />
The University of British Columbia | 
2016 - 2021 |  GPA: 88% <br />
International Student ‚Äì Faculty of Science Scholarship &amp; Dean‚Äôs Honour List</p>

<h2 id="-projects">üé≥ Projects</h2>

<p><strong>Visual Commonsense Generation &amp; its incorporation into a Multimodal Topic Modeling algorithm</strong> | December 2022 <br />
The task of commonsense knowledge generation is largely limited to the language domain, with models such as COMET (for explicit knowledge) and GPT-3 (for implicit knowledge). Moreover, VisualCOMET, a commonsense generation model that utilizes the visual context, is limited to three people-centric relations. Since commonsense generation on entire scenes, or parts of a scene, can be helpful in several downstream multimodal tasks, including VQA and topic modeling, we propose a general-purpose visual commonsense generation model, VisualCOMET+, by extending VisualCOMET with four diverse inference relations. Using the clue-rationale pairs from a visual abductive reasoning dataset, we train our commonsense generation model by creating groundtruth structured commonsense triplets. Then, we show that we can get coherent and more diverse topics by incorporating generated commonsense inferences and visual features into a novel multimodal topic modeling algorithm, Multimodal CTM. <br />
<a href="https://drive.google.com/file/d/1_HxrSJzDZKj1uDm7irCx3_9KnXlT7My-/view?usp=sharing">Report</a></p>

<p><strong>Commonsense reasoning in Visual Question Answering (VQA)</strong> <br />
Visual Question Answering with commonsense reasoning is a challenging task that requires models to understand of the image, the question, and contextualized commonsense knowledge to assist with the reasoning required to arrive at an answer. In our work, we propose extensions to the VLC-BERT, aimed at solving two drawbacks of the model by identifying potential words in the input sequence that may answer the question using a Pointer Generator, and incorporating additional image information in the form of object tags from an object detection model. Our evaluation shows that the Pointer Generator and object detection models help achieve higher scores on the OK-VQA dataset. Furthermore, we generate answers using GPT-3 and incorporate them into VLC-BERT. Our error analysis on GPT-3 and VLC-BERT models highlight that GPT-3 contains valuable implicit commonsense and factual knowledge that is beneficial to our model. <br />
<a href="https://drive.google.com/file/d/1eH1TtFI5QLS78mf7wWRcrUyZO6T3_N0a/view?usp=sharing">Report</a></p>

<p><strong>Graph-enhanced Transformers for Referring Expressions Comprehension</strong> <br />
We explore a simple method to incorporate inter-token relationships in a Transformer before performing any training, using graphs with edge features. In VL-BERT-Graph, we generate a fully-connected graph of input tokens where the edges represent similarity between the tokens, obtained using GloVE and CLIP. We then use a message-passing GNN to incorporate these features into the input tokens or the output encoding of the model, and train the Transformer with edge-feature attention masks. <br />
<a href="https://lrjconan.github.io/DL-structures/assets/sample_reports_2021/report_05.pdf">Report</a></p>

<p><strong>Learning faster Genetic Algorithms with dynamic mutation power</strong> <br />
Policy Gradient (PG) methods and Genetic Algorithms (GA) are used to train Reinforcement Learning agents to perform a particular task in an environment by maximizing the received reward. In the context of this assignment, both techniques aim to approximate a policy function that, given a state, produces a policy to pick the best action to maximize reward. Here, the policy function used is deep neural network model. In this project, I implement a PG method, REINFORCE, and a simple GA method to solve the Lunar Lander (LunarLander-v2) environment in OpenAI Gym. I propose two modifications to the GA method: an improved fitness function with which the GA can solve the task in about 50 generations, and a novel dynamic mutation power technique that helps the model solve the task in 30 generations. <br />
<a href="https://youtu.be/BmMubRYbuQM">Video</a> | <a href="https://drive.google.com/file/d/1bsnn7sfDrHZSZhAxYsIKkBmPxyytd4Xk/view?usp=sharing">Report</a></p>

<p><strong>A Summary of Recent Text Summarization Techniques</strong> <br />
In this project paper, we surveyed text summarization models by evaluating existing extractive and abstractive models. We studied the metrics and datasets used to evaluate the latest models and evaluated upcoming abstractive techniques. Finally, we highlighted future pathways for text summarization and suggested areas for improvement. <br />
<a href="https://drive.google.com/file/d/1ayX-OSNrvvJsNsnVA_16JzFIVmI0NnoB/view">Report</a></p>

<p><strong>Universal Machine Learning API</strong> <br />
A powerful Python API template, built on Flask, for plug-and-play use with machine learning models. <br />
<em>Technologies used: Python, with the Flask API package</em> <br />
<a href="https://medium.com/technonerds/a-production-grade-machine-learning-api-using-flask-gunicorn-nginx-and-docker-part-1-49927238befb">Blog</a> | <a href="https://github.com/aditya10/flask-ml-api">Github</a></p>

<p><strong>BERT Transformer based Text Classifier using TensorFlow</strong> <br />
A multi-class text classification example on the StackOverflow Questions dataset. <br />
<em>Technologies used: Python, with Pandas, TensorFlow</em> <br />
<a href="https://medium.com/technonerds/using-fastais-ulmfit-to-make-a-state-of-the-art-multi-label-text-classifier-bf54e2943e83">Blog</a></p>
:ET