---
layout: post
title:  "VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge"
date:   2022-10-24 09:29:20 -0800
categories: projects
category: publication
authors: "Sahithya Ravi*, Aditya Chinchure*, Leonid Sigal, Renjie Liao, Vered Shwartz (*equal contribution)"
conference: "WACV 2023"
image: assets/posts/vlc-bert/vlc-bert.png
description: We present a new Vision-Language-Commonsense transformer model, VLC-BERT, that incorporates contextualized knowledge using Commonsense Transformer (COMET) to solve Visual Question Answering (VQA) tasks that require commonsense reasoning.
---
We present a new Vision-Language-Commonsense transformer model, VLC-BERT, that incorporates contextualized knowledge using Commonsense Transformer (COMET) to solve Visual Question Answering (VQA) tasks that require commonsense reasoning. VLC-BERT outperforms existing models that utilize static knowledge bases, and the article provides a detailed analysis of which questions benefit from the contextualized commonsense knowledge from COMET.\
[Paper](https://openaccess.thecvf.com/content/WACV2023/papers/Ravi_VLC-BERT_Visual_Question_Answering_With_Contextualized_Commonsense_Knowledge_WACV_2023_paper.pdf) | [Github](https://github.com/aditya10/VLC-BERT) | [ArXiv](https://arxiv.org/abs/2210.13626) 