<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2023-05-03T17:59:31-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Aditya Chinchure</title><subtitle>Graduate Student at University of British Columbia</subtitle><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><entry><title type="html">DE-TensoRF: Data-efficient and fast NeRFs</title><link href="http://localhost:4000/de-tensorf/" rel="alternate" type="text/html" title="DE-TensoRF: Data-efficient and fast NeRFs" /><published>2023-04-28T19:29:20-07:00</published><updated>2023-04-28T19:29:20-07:00</updated><id>http://localhost:4000/de-tensorf</id><content type="html" xml:base="http://localhost:4000/de-tensorf/"><![CDATA[<p>Developed DE-TensoRF, a model that can render 3D objects with as few as 3 images, and in under 15 min on a single GPU. We achieved the highest grade in our class, and led to collaboration efforts with Dr. Helge Rhodin’s research group.</p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="academic" /><summary type="html"><![CDATA[Developed DE-TensoRF, a model that can render 3D objects with as few as 3 images, and in under 15 min on a single GPU. We achieved the highest grade in our class, and led to collaboration efforts with Dr. Helge Rhodin’s research group.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/de-tensorf/detensorf.gif" /><media:content medium="image" url="http://localhost:4000/assets/posts/de-tensorf/detensorf.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">VisualCOMET+: Visual Commonsense Generation &amp;amp; its incorporation into a Multimodal Topic Modeling algorithm</title><link href="http://localhost:4000/viscomet+/" rel="alternate" type="text/html" title="VisualCOMET+: Visual Commonsense Generation &amp;amp; its incorporation into a Multimodal Topic Modeling algorithm" /><published>2022-12-09T18:29:20-08:00</published><updated>2022-12-09T18:29:20-08:00</updated><id>http://localhost:4000/viscomet+</id><content type="html" xml:base="http://localhost:4000/viscomet+/"><![CDATA[<p>The task of commonsense knowledge generation is largely limited to the language domain, with models such as COMET (for explicit knowledge) and GPT-3 (for implicit knowledge). Moreover, VisualCOMET, a commonsense generation model that utilizes the visual context, is limited to three people-centric relations. Since commonsense generation on entire scenes, or parts of a scene, can be helpful in several downstream multimodal tasks, including VQA and topic modeling, we propose a general-purpose visual commonsense generation model, VisualCOMET+, by extending VisualCOMET with four diverse inference relations. Using the clue-rationale pairs from a visual abductive reasoning dataset, we train our commonsense generation model by creating groundtruth structured commonsense triplets. Then, we show that we can get coherent and more diverse topics by incorporating generated commonsense inferences and visual features into a novel multimodal topic modeling algorithm, Multimodal CTM. <br />
<a href="https://drive.google.com/file/d/1_HxrSJzDZKj1uDm7irCx3_9KnXlT7My-/view?usp=sharing">Report</a></p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="academic" /><summary type="html"><![CDATA[Developed an extension to VisualCOMET to generate general-purpose commonsense knowledge from images. Showed improvements on coherence and diversity scores of a novel topic modelling algorithm using the generated knowledge]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/commonsense-gen/viscomet+.png" /><media:content medium="image" url="http://localhost:4000/assets/posts/commonsense-gen/viscomet+.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge</title><link href="http://localhost:4000/vlc-bert/" rel="alternate" type="text/html" title="VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" /><published>2022-10-23T19:29:20-07:00</published><updated>2022-10-23T19:29:20-07:00</updated><id>http://localhost:4000/vlc-bert</id><content type="html" xml:base="http://localhost:4000/vlc-bert/"><![CDATA[<p>We present a new Vision-Language-Commonsense transformer model, VLC-BERT, that incorporates contextualized knowledge using Commonsense Transformer (COMET) to solve Visual Question Answering (VQA) tasks that require commonsense reasoning. VLC-BERT outperforms existing models that utilize static knowledge bases, and the article provides a detailed analysis of which questions benefit from the contextualized commonsense knowledge from COMET.</p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="publication" /><summary type="html"><![CDATA[We present a new Vision-Language-Commonsense transformer model, VLC-BERT, that incorporates contextualized knowledge using Commonsense Transformer (COMET) to solve Visual Question Answering (VQA) tasks that require commonsense reasoning.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/vlc-bert/vlc-bert.png" /><media:content medium="image" url="http://localhost:4000/assets/posts/vlc-bert/vlc-bert.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">VL-BERT-Graph: Graph-enhanced Transformers for Referring Expressions Comprehension</title><link href="http://localhost:4000/vl-bert-graph/" rel="alternate" type="text/html" title="VL-BERT-Graph: Graph-enhanced Transformers for Referring Expressions Comprehension" /><published>2022-04-28T19:29:20-07:00</published><updated>2022-04-28T19:29:20-07:00</updated><id>http://localhost:4000/vl-bert-graph</id><content type="html" xml:base="http://localhost:4000/vl-bert-graph/"><![CDATA[<p>We explore a simple method to incorporate inter-token relationships in a Transformer before performing any training, using graphs with edge features. In VL-BERT-Graph, we generate a fully-connected graph of input tokens where the edges represent similarity between the tokens, obtained using GloVE and CLIP. We then use a message-passing GNN to incorporate these features into the input tokens or the output encoding of the model, and train the Transformer with edge-feature attention masks. <br />
<a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/assets/sample_reports_2021_W2/report_05.pdf">Report</a></p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="academic" /><summary type="html"><![CDATA[Incorporated Graph Neural Networks in a visual-linguistic Transformer]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/vl-bert-graph/vl-bert-graph.png" /><media:content medium="image" url="http://localhost:4000/assets/posts/vl-bert-graph/vl-bert-graph.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Universal Machine Learning API</title><link href="http://localhost:4000/universal-ml-api/" rel="alternate" type="text/html" title="Universal Machine Learning API" /><published>2022-04-28T19:29:20-07:00</published><updated>2022-04-28T19:29:20-07:00</updated><id>http://localhost:4000/universal-ml-api</id><content type="html" xml:base="http://localhost:4000/universal-ml-api/"><![CDATA[<p><strong>Universal Machine Learning API</strong> <br />
A powerful Python API template, built on Flask, for plug-and-play use with machine learning models. <br />
<em>Technologies used: Python, with the Flask API package</em> <br />
<a href="https://medium.com/technonerds/a-production-grade-machine-learning-api-using-flask-gunicorn-nginx-and-docker-part-1-49927238befb">Blog</a> | <a href="https://github.com/aditya10/flask-ml-api">Github</a></p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="other" /><summary type="html"><![CDATA[A powerful Python API template, built on Flask, for plug-and-play use with machine learning models.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/vl-bert-graph/vl-bert-graph.png" /><media:content medium="image" url="http://localhost:4000/assets/posts/vl-bert-graph/vl-bert-graph.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Investigating extensions to VLC-BERT and comparing it with GPT-3</title><link href="http://localhost:4000/vlc-bert-ext/" rel="alternate" type="text/html" title="Investigating extensions to VLC-BERT and comparing it with GPT-3" /><published>2022-04-20T19:29:20-07:00</published><updated>2022-04-20T19:29:20-07:00</updated><id>http://localhost:4000/vlc-bert-ext</id><content type="html" xml:base="http://localhost:4000/vlc-bert-ext/"><![CDATA[<p>Visual Question Answering with commonsense reasoning is a challenging task that requires models to understand of the image, the question, and contextualized commonsense knowledge to assist with the reasoning required to arrive at an answer. In our work, we propose extensions to the VLC-BERT, aimed at solving two drawbacks of the model by identifying potential words in the input sequence that may answer the question using a Pointer Generator, and incorporating additional image information in the form of object tags from an object detection model. Our evaluation shows that the Pointer Generator and object detection models help achieve higher scores on the OK-VQA dataset. Furthermore, we generate answers using GPT-3 and incorporate them into VLC-BERT. Our error analysis on GPT-3 and VLC-BERT models highlight that GPT-3 contains valuable implicit commonsense and factual knowledge that is beneficial to our model. <br />
<a href="https://drive.google.com/file/d/1eH1TtFI5QLS78mf7wWRcrUyZO6T3_N0a/view?usp=sharing">Report</a></p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="academic" /><summary type="html"><![CDATA[This project extends VLC-BERT with pointer generator networks and object detection models. Furthermore, we compare the performance of VLC-BERT with GPT-3 on the OK-VQA dataset.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/vlc-bert-extended/vlc-bert-extended.png" /><media:content medium="image" url="http://localhost:4000/assets/posts/vlc-bert-extended/vlc-bert-extended.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Learning faster Genetic Algorithms with dynamic mutation power</title><link href="http://localhost:4000/growing-agents/" rel="alternate" type="text/html" title="Learning faster Genetic Algorithms with dynamic mutation power" /><published>2021-12-03T18:29:20-08:00</published><updated>2021-12-03T18:29:20-08:00</updated><id>http://localhost:4000/growing-agents</id><content type="html" xml:base="http://localhost:4000/growing-agents/"><![CDATA[<p>Policy Gradient (PG) methods and Genetic Algorithms (GA) are used to train Reinforcement Learning agents to perform a particular task in an environment by maximizing the received reward. In the context of this assignment, both techniques aim to approximate a policy function that, given a state, produces a policy to pick the best action to maximize reward. Here, the policy function used is deep neural network model. In this project, I implement a PG method, REINFORCE, and a simple GA method to solve the Lunar Lander (LunarLander-v2) environment in OpenAI Gym. I propose two modifications to the GA method: an improved fitness function with which the GA can solve the task in about 50 generations, and a novel dynamic mutation power technique that helps the model solve the task in 30 generations. <br />
<a href="https://youtu.be/BmMubRYbuQM">Video</a> | <a href="https://drive.google.com/file/d/1bsnn7sfDrHZSZhAxYsIKkBmPxyytd4Xk/view?usp=sharing">Report</a></p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="academic" /><summary type="html"><![CDATA[This project introduces a modification to the GA algorithm to introduce dynamic mutation power, to solve the Lunar Lander evironment on OpenAI Gym in 30 generations.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/growing-agents/ga.png" /><media:content medium="image" url="http://localhost:4000/assets/posts/growing-agents/ga.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Summary of Recent Text Summarization Techniques</title><link href="http://localhost:4000/text-summarization/" rel="alternate" type="text/html" title="A Summary of Recent Text Summarization Techniques" /><published>2020-12-03T18:29:20-08:00</published><updated>2020-12-03T18:29:20-08:00</updated><id>http://localhost:4000/text-summarization</id><content type="html" xml:base="http://localhost:4000/text-summarization/"><![CDATA[<p>In this project paper, we surveyed text summarization models by evaluating existing extractive and abstractive models. We studied the metrics and datasets used to evaluate the latest models and evaluated upcoming abstractive techniques. Finally, we highlighted future pathways for text summarization and suggested areas for improvement. <br />
<a href="https://drive.google.com/file/d/1ayX-OSNrvvJsNsnVA_16JzFIVmI0NnoB/view">Report</a></p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="academic" /><summary type="html"><![CDATA[In this project paper, we surveyed text summarization models by evaluating existing extractive and abstractive models. We studied the metrics and datasets used to evaluate the latest models and evaluated upcoming abstractive techniques. Finally, we highlighted future pathways for text summarization and suggested areas for improvement]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/text-summarization/text-summarization.png" /><media:content medium="image" url="http://localhost:4000/assets/posts/text-summarization/text-summarization.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2020-08-28T19:29:20-07:00</published><updated>2020-08-28T19:29:20-07:00</updated><id>http://localhost:4000/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/welcome-to-jekyll/"><![CDATA[<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name>Aditya Chinchure</name><email>aditya10@cs.ubc.ca</email></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.]]></summary></entry></feed>